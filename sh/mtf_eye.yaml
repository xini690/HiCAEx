# Training parameters
epochs: 50
batch_size: 32 #32
blr: 5e-4
layer_decay: 0.65
drop_path: 0.2
min_lr: 1e-6
warmup_epochs: 10
input_size: 224
weight_decay: 0.05
use_mean_pooling: True
init_scale: 0.001

savemodel: True


# checkpoint
cfp_model: "/weight/RETFound_cfp_weights.pth"
oct_model: "/weight/RETFound_oct_weights.pth"

multi_label_classification: False

nb_classes: 3  #16 #8 #5 #3

# dataset
csv_path: "/dataSet/AMD/trainValTest/"
image_path: "/dataSet/AMD/images/"
output_dir: "/output/"


return_all_tokens: True # HMLTF
use_moe: True # MAAER
images_number: 2 # number of modalities


#return_all_tokens: True
#use_moe: False
#images_number: 1
#in_domains: "oct"  #"cfp"


num_experts: 4
label_column: "label"
base_alpha: 0.1
loss_alpha: 0.1
multi_label_threshold: 0.5
transform_type: "mtf_eye"

# Wandb logging
log_wandb: False
log_tensorboard: False
wandb_project: 'hiCAEx-finetune-cls'
wandb_entity: null # Change if needed
wandb_run_name: hiCAEx-test

save_confusion_matrix: True

# Architecture
model: multivit_large
patch_size: 16
num_global_tokens: 1
world_size: 1
